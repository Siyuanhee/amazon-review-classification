{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siyuan\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Siyuan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim import corpora\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"amazon_review_large.csv\", \"r\") as f:\n",
    "     \n",
    "    csvfile = csv.reader(f, delimiter=',')\n",
    "\n",
    "    label,text=zip(*csvfile)\n",
    "\n",
    "# convert tuple to list\n",
    "text=list(text)\n",
    "label=list(label)\n",
    "for i, x in enumerate(label):\n",
    "    if x == '1': label[i] = 0\n",
    "    else: label[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# set the maximum number of words to be used\n",
    "MAX_NB_WORDS=10000\n",
    "\n",
    "# set sentence/document length\n",
    "MAX_DOC_LEN=500\n",
    "\n",
    "# get a Keras tokenizer\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "# get the mapping between word and its index\n",
    "voc=tokenizer.word_index\n",
    "\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "              \n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.01):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z=Concatenate(name='concate')(conv_blocks)\n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(192, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      " - 88s - loss: 0.9119 - acc: 0.6996 - val_loss: 0.4123 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83650, saving model to best_model\n",
      "Epoch 2/20\n",
      " - 85s - loss: 0.3560 - acc: 0.8598 - val_loss: 0.3999 - val_acc: 0.8363\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.83650\n",
      "Epoch 3/20\n",
      " - 83s - loss: 0.2257 - acc: 0.9284 - val_loss: 0.3749 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.83650 to 0.85933, saving model to best_model\n",
      "Epoch 4/20\n",
      " - 88s - loss: 0.1431 - acc: 0.9623 - val_loss: 0.4140 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85933\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "NUM_OUTPUT_UNITS=2\n",
    "MAX_NB_WORDS=10000\n",
    "MAX_DOC_LEN=500\n",
    "# filters on bigrams, trigrams, and 4-grams\n",
    "FILTER_SIZES=[2,3,4]\n",
    "BEST_MODEL_FILEPATH='best_model'\n",
    "\n",
    "BATCH_SIZES = 64\n",
    "NUM_EPOCHES = 20\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences, label, test_size=0.3, random_state=0)\n",
    "\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, MAX_DOC_LEN)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BATCH_SIZES, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 94.42%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"best_model\")\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(padded_sequences, label, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94837356]\n",
      " [ 0.02075764]\n",
      " [ 0.40209559]\n",
      " [ 0.63034117]\n",
      " [ 0.18376663]\n",
      " [ 0.89941788]\n",
      " [ 0.48391566]\n",
      " [ 0.85834521]\n",
      " [ 0.84794623]\n",
      " [ 0.51868761]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
